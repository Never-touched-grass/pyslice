# The following file was generated by pyslice
import re

class Token:
    def __init__(self, type_, lexeme):
        self.type = type_
        self.lexeme = lexeme

    def __repr__(self):
        return f"Token({self.type}, '{self.lexeme}')"

class Lexer:
    def __init__(self, inp, verbose=False):
        self.text = inp
        self.pos = 0
        self.verbose = verbose
        self.token_defs = [
        ('NUM', re.compile(r'[0-9]+'), False),
        ('VAR', re.compile(r'var'), False),
        ('IDENT', re.compile(r'[a-zA-Z_][a-zA-Z0-9_]*'), False),
        ('EQ', re.compile(r'\='), False),
        ('WHITESPACE', re.compile(r'[ \t\n\r]'), True),
        ('COMMENT', re.compile(r'//.*'), True),
        ('PLUS', re.compile(r'\+'), False),
        ('MINUS', re.compile(r'\-'), False),
        ('SLASH', re.compile(r'/'), False),
        ('STAR', re.compile(r'\*'), False),
    ]

    def next_token(self):
        if self.pos >= len(self.text):
            return None
        text = self.text[self.pos:]
        for name, pattern, ignore in self.token_defs:
            match = pattern.match(text)
            if match:
                lexeme = match.group(0)
                self.pos += len(lexeme)
                if ignore:
                    return self.next_token()
                if self.verbose:
                    self.print_verbose(name, lexeme)
                return Token(name, lexeme)
        raise ValueError(f"Unexpected character at position {self.pos}: '{text[0]}')")
    def tokenize(self):
        tokens = []
        while True:
            tok = self.next_token()
            if tok is None:
                break
            tokens.append(tok)
        return tokens
    def print_verbose(self, name, lexeme):
        print(f"[VERBOSE] Matched {name}: '{lexeme}'")      
        
